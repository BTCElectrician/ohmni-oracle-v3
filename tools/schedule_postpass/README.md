# Schedule Post-Pass (Unified Index)

## TL;DR
- Keep **SHEET** docs as-is (current per-sheet JSON + `content`).
- Emit one **FACT** doc per **schedule row** into the **same index** (`doc_type="fact"`).
- Emit one **TEMPLATE** doc per room using the `templates/e_rooms_template.json` + `templates/a_rooms_template.json` scaffolds (foreman editable, `doc_type="room"`).
- Index name: `drawings_unified`.

## Module Structure

The `transform.py` script has been refactored into focused modules:

- **`io_utils.py`**: JSON/JSONL file I/O (`load_json`, `write_jsonl`)
- **`ids.py`**: Document ID generation and key sanitization (`make_document_id`, `sanitize_key_component`, `stable_key`, `_is_bogus_room`)
- **`embeddings.py`**: Vector embedding generation (`generate_embedding`, `create_embedding_client`)
- **`metadata.py`**: Sheet metadata extraction (`sheet_meta`, `make_sheet_doc`, `_to_iso_date`)
- **`facts.py`**: Fact document generation from schedule blocks (`emit_facts`)
- **`template_docs.py`**: Template document generation (`iter_template_docs`, `_collect_sheet_templates`, `_guess_template_type`)
- **`coverage.py`**: Coverage report generation (`coverage_rows`)
- **`transform.py`**: CLI entrypoint and orchestration

The CLI interface and output format remain unchanged. All modules can be imported individually for programmatic use.

## 1) Run the transformer
Room templates are generated by the processing pipeline and live under `processed/room-data/<drawing_slug>/` (e.g., `processed/room-data/E2.03/E2.03_e_rooms_details.json` and `E2.03_a_rooms_details.json`). The transformer recursively searches for all `*.json` files under the templates root.

```bash
python3 tools/schedule_postpass/transform.py \
  /path/to/sheet_json_folder \
  /tmp/out \
  veridian \
  --templates-root processed/room-data
```

Or as a module:

```bash
python3 -m tools.schedule_postpass.transform \
  /path/to/sheet_json_folder \
  /tmp/out \
  veridian \
  --templates-root processed/room-data
```

**Note:** The `--templates-root` should point to the base `room-data` folder (or the parent directory containing `room-data/`). The transformer will recursively find all `*_rooms_details.json` files under this path.

Generates embeddings for vector search using OpenAI `text-embedding-3-large`.
Embedding cost: ~$0.78 per 12-story building (one-time, updates only changed docs).

To update *only* foreman-edited room templates (skipping sheet/fact reprocessing):

```bash
python3 tools/schedule_postpass/transform.py \
  /path/to/sheet_json_folder \
  /tmp/out \
  veridian \
  --templates-root processed/room-data \
  --templates-only
```

Outputs:

- `/tmp/out/sheets.jsonl`
- `/tmp/out/facts.jsonl`
- `/tmp/out/templates.jsonl`
- `/tmp/out/coverage_report.csv` (expanded metrics + template coverage)

## 2) Create the index & upload docs

Set env vars:

```bash
export AZURE_SEARCH_ENDPOINT="https://<service>.search.windows.net"
export AZURE_SEARCH_API_KEY="<key>"
export INDEX_NAME="drawings_unified"
```

Create + upload (Full rebuild):

```bash
python tools/schedule_postpass/upsert_index.py \
  --schema ./tools/schedule_postpass/unified_index.schema.json \
  --synonyms ./tools/schedule_postpass/synonyms.seed.json \
  --sheets /tmp/out/sheets.jsonl \
  --facts  /tmp/out/facts.jsonl \
  --templates /tmp/out/templates.jsonl \
  --mode full
```

Create + upload (Incremental template sync):

```bash
python tools/schedule_postpass/upsert_index.py \
  --schema ./tools/schedule_postpass/unified_index.schema.json \
  --templates /tmp/out/templates.jsonl \
  --mode incremental
```

> When a foreman tweaks a single room, rerun `transform.py --templates-only ...` to refresh `/tmp/out/templates.jsonl`, then call the uploader with `--mode incremental` so you don’t recycle the whole index mid-day.

## 3) Attach synonyms

Passing `--synonyms ./tools/schedule_postpass/synonyms.seed.json` to `upsert_index.py` auto-creates/updates the `project-synonyms` map. If you prefer to manage maps in the portal, follow the manual steps below.

1. In the Azure Portal, open your AI Search service.
2. Go to **Synonym maps**.
3. Create a new map (e.g., `project-synonyms-map`).
4. Upload/copy the contents of `synonyms.seed.json` into it.
5. Go to your `drawings_unified` **Index** → **Fields** and select the searchable fields (`content`, `project`, `sheet_number`, `sheet_title`, `key/panel`, `key/tag`, `attributes/description`, `room_id`, `room_name`, `template_tags`, etc.).
6. In the **Synonym maps** dropdown for those fields, select your new `project-synonyms-map` and save.

## 4) Quick checks (Developer Sanity Test)

> `query_playbook.py` is a simple **developer sanity check**. Production query logic (facts-first with sheets fallback) remains in your Azure Function. This script just validates that `transform.py` and `upsert_index.py` worked.

```bash
python tools/schedule_postpass/query_playbook.py
```

Expected checks:

- Dishwasher in Unit A4 → a `unit_plan` FACT
- Signed-off templates in Unit A4 → a `template` doc
- Panel S2 schedule → `panel` FACTs (from the filter)
- Main Riser Diagram → likely a `sheet` doc as fallback

## 5) Import order (recommended for full rebuild)

1. Electrical **panel schedules** (all rows)
2. **Unit plan schedule** (appliances: DW/Range/WH/Dryer)
3. Mechanical **equipment** schedule
4. **Lighting** fixture schedule
5. Architectural: **wall/partition**, **door**, **ceiling**, **finish**
6. **Templates** (must be last so the latest foreman edit is indexed)

## 6) Fact coverage checklist
- Extractor output (`*_structured.json`) is the canonical source-of-truth. It lives in blob storage unchanged.
- `transform.py` derives `sheets.jsonl`, `facts.jsonl`, `templates.jsonl` from that structured payload. Azure AI Search only ingests these JSONL files (one document per row) so we keep the index fast and filterable.
- Fallback iterators in `tools/schedule_postpass/fallbacks/` synthesize facts when a sheet JSON lacks top-level `blocks/rows`:
  - Electrical: `ELECTRICAL.panels[].circuits`, `ELECTRICAL.PANEL_SCHEDULES`
  - Mechanical: `MECHANICAL.equipment` (dict-of-lists or flat list)
  - Plumbing: `PLUMBING.fixtures`, `PLUMBING.water_heaters|waterHeaters`
  - Architectural: wall/partition, door, ceiling, finish schedule keys
- To sanity-check coverage on a new project:
  1. Pick a structured JSON file and locate a schedule section (e.g., `ELECTRICAL.panels[].circuits`). Count rows or just skim the entries you care about.
  2. Open the matching `facts.jsonl` and filter by `schedule_type` + `sheet_number`. Every row from the structured JSON should have a matching fact.
  3. Optional quick count script:

    ```python
    import json

    with open("E5.00_structured.json") as f:
        raw = json.load(f)
    num_circuits = sum(len(panel.get("circuits", [])) for panel in raw.get("ELECTRICAL", {}).get("panels", []))

    with open("facts.jsonl") as f:
        facts = [json.loads(line) for line in f]
    num_panel_facts = sum(1 for fact in facts if fact["schedule_type"] == "panel" and fact["sheet_number"] == "E5.00")

    print(num_circuits, num_panel_facts)
    ```

- If counts diverge, copy the relevant JSON snippet and extend the appropriate fallback iterator with new key aliases. Most tweaks are <10 LOC.
- Always keep files under ~500 lines; each discipline iterator lives in its own module for that reason.

## 7) Extending fallbacks on new drawing sets

When a fresh project introduces a schedule shape we have not seen, follow this loop:

1. **Extract first.** Run the main pipeline so the `_structured.json` lands under `processed/<discipline>/<sheet>/`.
2. **Inspect the JSON.** Focus on schedule-like sections (anything ending with `_SCHEDULE`, lists named `fixtures`, `fans`, `devices`, etc.). Note the key that should map to `tag`, `panel`, `circuit`, etc.
3. **Update the iterator.** Edit the matching module in `tools/schedule_postpass/fallbacks/` to include the new key aliases. Keep the edit scoped (<10 LOC) and continue normalizing identifiers (e.g., copy `DESIG.` into `tag`).
4. **Regenerate payloads.** Run `make index-pack SOURCE=/absolute/path/to/processed` so `facts.jsonl` reflects the change. The coverage report also refreshes here.
5. **Confirm counts.** Use the quick script above or open `coverage_report.csv`—the sheet’s raw row count should equal the fact count.
6. **Rebuild or upsert.** Once satisfied, `make index-rebuild SOURCE=... TEMPLATES_ROOT=...` (full upload) or `python tools/schedule_postpass/upsert_index.py ... --mode incremental` (partial update).

Common aliases we already handle:

- Electrical: `panel_id`, `ckt`, `enclosure_info.volts`, `panel_schedules`, `panel_schedules[]`
- Mechanical: any key ending in `_SCHEDULE` whose value is a dict with `fans`/`devices`/`units` lists; `DESIG.` normalized to `tag`
- Plumbing: `PLUMBING_FIXTURE_SCHEDULE`, `ELECTRIC_WATER_HEATER_SCHEDULE`, `MARK` → `tag`
- Architectural: wall/partition, door, ceiling, finish schedule arrays with aliased fields (`partition_type`, `door_number`, etc.)

If counts drop to zero unexpectedly, double-check the iterator for typos and verify the extractor didn’t change naming conventions.

## 8) Quick command reference (developer return checklist)

```bash
# 1. Regenerate payloads from a specific processed folder
make index-pack SOURCE=/Users/<you>/<job>/processed \
    TEMPLATES_ROOT=/Users/<you>/<job>/processed/room-data

# 2. Full index rebuild (recreates index + uploads all docs)
make index-rebuild SOURCE=/Users/<you>/<job>/processed \
    TEMPLATES_ROOT=/Users/<you>/<job>/processed/room-data

# 3. Inspect latest facts
head tmp/index_out/facts.jsonl

# 4. Compare structured rows vs facts for a sheet
python - <<'PY'
import json, pathlib
facts = [json.loads(line) for line in open('tmp/index_out/facts.jsonl')]
sheet = 'M6.01'
fact_rows = [f for f in facts if f['sheet_number'] == sheet]
structured = pathlib.Path('/Users/<you>/<job>/processed').rglob(f"{sheet}*_structured.json")
struct_path = next(structured)
raw = json.load(open(struct_path))
mech = raw.get('MECHANICAL', {})
total = 0
for key, val in mech.items():
    if isinstance(val, dict):
        for kid in ('fans','devices','units','equipment','items','rows'):
            nested = val.get(kid)
            if isinstance(nested, list):
                total += sum(1 for item in nested if isinstance(item, dict))
print(f"facts: {len(fact_rows)} raw_rows: {total}")
PY

# 5. If you only need semantic Q&A, skip facts and chunk the structured JSON
```

> MVP note: For a fast demo, you can directly chunk the structured JSON files (no fallback edits required) and load those chunks into your vector index. Facts remain useful when you want structured filters or faceting later.
